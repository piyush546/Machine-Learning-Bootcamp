,Name,Author,Abstract,Download link
0,Quantifying Interpretability and Trust in Machine Learning Systems,"['Philipp Schmidt', 'Felix Biessmann']","Decisions by Machine Learning (ML) models have become ubiquitous. Trusting these decisions requires understanding how algorithms take them. Hence interpretability methods for ML are an active focus of research. A central problem in this context is that both the quality of interpretability methods as well as trust in ML predictions are difficult to measure. Yet evaluations, comparisons and improvements of trust and interpretability require quantifiable measures. Here we propose a quantitative measure for the quality of interpretability methods. Based on that we derive a quantitative measure of trust in ML decisions. Building on previous work we propose to measure intuitive understanding of algorithmic decisions using the information transfer rate at which humans replicate ML model predictions. We provide empirical evidence from crowdsourcing experiments that the proposed metric robustly differentiates interpretability methods. The proposed metric also demonstrates the value of interpretability for ML assisted human decision making: in our experiments providing explanations more than doubled productivity in annotation tasks. However unbiased human judgement is critical for doctors, judges, policy makers and others. Here we derive a trust metric that identifies when human decisions are overly biased towards ML predictions. Our results complement existing qualitative work on trust and interpretability by quantifiable measures that can serve as objectives for further improving methods in this field of research.
        ? Less",https://arxiv.org/pdf/1901.08558
1,When Machine Learning Meets Big Data: A Wireless Communication Perspective,"['Yuanwei Liu', 'Suzhi Bi', 'Zhiyuan Shi', 'Lajos Hanzo']","We have witnessed an exponential growth in commercial data services, which has lead to the big data's era. Machine learning, as one of the most promising artificial intelligence tools of analyzing the deluge of data, has been invoked in many research areas both in academia and industry. The aim of this article is twin-fold. Firstly, we review big data analysis and machine learning, along with their potential applications in wireless networks. The second goal is to invoke big data analysis to predict the requirements of mobile users and to exploit it for improving the performance of wireless networks. More particularly, a unified big data aided machine learning framework is proposed, which consists of feature extraction, data modeling and prediction/online refinement. The main benefits of the proposed framework are that by relying on big data which reflects the real requirements of users, we can refine the motivation, problem formulations, and methodology of powerful machine learning algorithms in the context of wireless networks. To characterize the efficiency of the proposed framework, a pair of practical case studies are provided: 1) To predict the positioning of drone-mounted areal base stations (BSs) deployments according to specific tele-traffic requirements. 2) To predict the content caching requirements of BSs according to the users' preferences. Finally, open research opportunities are identified for motivating future investigations.
        ? Less",https://arxiv.org/pdf/1901.08329
2,Machine Learning and Deep Learning Algorithms for Bearing Fault Diagnostics - A Comprehensive Review,"['Shen Zhang', 'Shibo Zhang', 'Bingnan Wang', 'Thomas G. Habetler']","In this survey paper, we systematically summarize the current literature on studies that apply machine learning (ML) and data mining techniques to bearing fault diagnostics. Conventional ML methods, including artificial neural network (ANN), principal component analysis (PCA), support vector machines (SVM), etc., have been successfully applied to detecting and categorizing bearing faults since the last decade, while the application of deep learning (DL) methods has sparked great interest in both the industry and academia in the last five years. In this paper, we will first review the conventional ML methods, before taking a deep dive into the latest developments in DL algorithms for bearing fault applications. Specifically, the superiority of the DL based methods over the conventional ML methods are analyzed in terms of metrics directly related to fault feature extraction and classifier performances; the new functionalities offered by DL techniques that cannot be accomplished before are also summarized. In addition, to obtain a more intuitive insight, a comparative study is performed on the classifier performance and accuracy for a number of papers utilizing the open source Case Western Reserve University (CWRU) bearing data set. Finally, based on the nature of the time-series 1-D data obtained from sensors monitoring the bearing conditions, recommendations and suggestions are provided to applying DL algorithms on bearing fault diagnostics based on specific applications, as well as future research directions to further improve its performance.
        ? Less",https://arxiv.org/pdf/1901.08247
3,PD-ML-Lite: Private Distributed Machine Learning from Lighweight Cryptography,"['Maksim Tsikhanovich', 'Malik Magdon-Ismail', 'Muhammad Ishaq', 'Vassilis Zikas']","Privacy is a major issue in learning from distributed data. Recently the cryptographic literature has provided several tools for this task. However, these tools either reduce the quality/accuracy of the learning algorithm---e.g., by adding noise---or they incur a high performance penalty and/or involve trusting external authorities.
  We propose a methodology for {\sl private distributed machine learning from light-weight cryptography} (in short, PD-ML-Lite). We apply our methodology to two major ML algorithms, namely non-negative matrix factorization (NMF) and singular value decomposition (SVD). Our resulting protocols are communication optimal, achieve the same accuracy as their non-private counterparts, and satisfy a notion of privacy---which we define---that is both intuitive and measurable. Our approach is to use lightweight cryptographic protocols (secure sum and normalized secure sum) to build learning algorithms rather than wrap complex learning algorithms in a heavy-cost MPC framework.
  We showcase our algorithms' utility and privacy on several applications: for NMF we consider topic modeling and recommender systems, and for SVD, principal component regression, and low rank approximation.
        ? Less",https://arxiv.org/pdf/1901.07986
4,Machine Learning for Wireless Communications in the Internet of Things: A Comprehensive Survey,"['Jithin Jagannath', 'Nicholas Polosky', 'Anu Jagannath', 'Francesco Restuccia', 'Tommaso Melodia']","The Internet of Things (IoT) is expected to require more effective and efficient wireless communications than ever before. For this reason, techniques such as spectrum sharing and dynamic spectrum access will soon become essential components of the IoT wireless communication process. In this vision, IoT devices must be able to not only learn to autonomously extract spectrum knowledge on-the-fly from the network but also leverage such knowledge to dynamically change appropriate wireless parameters (e.g., frequency band, symbol modulation, coding rate, route selection etc) to reach the network's optimal operating point. To address the above challenges, much research has been devoted to exploring the use of machine learning to address problems in the IoT wireless communications domain. The reason behind machine learning's popularity is that it provides a general framework to solve very complex problems where a model of the phenomenon being learned is too complex to derive or too dynamic to be summarized in mathematical terms. This work provides a comprehensive survey of the state of the art in the application of machine learning techniques to address key problems in IoT wireless communications with an emphasis on its ad hoc networking aspect. First, we present extensive background notions on machine learning techniques. Then, by adopting a bottom-up approach, we examine existing work on machine learning for the IoT at the physical, data-link and network layer of the protocol stack. Thereafter, we discuss directions taken by the community towards hardware implementation to ensure the feasibility of these techniques. Finally, we provide a series of research challenges associated with the applications of machine learning techniques for IoT wireless communications.
        ? Less",https://arxiv.org/pdf/1901.07947
5,Characterization of photoexcited states in the half-filled one-dimensional extended Hubbard model assisted by machine learning,"['Kazuya Shinjo', 'Shigetoshi Sota', 'Seiji Yunoki', 'Takami Tohyama']","Photoinduced nonequilibrium states can provide new insights into the dynamical properties in strongly correlated electron systems. One of the typical and extensively studied systems is the half-filled one-dimensional extended Hubbard model (1DEHM). Here, we propose that the supervised machine learning (ML) can give useful information for characterizing the photoexcited state in 1DEHM. Using entanglement spectra as a training dataset, we construct the neural network. Judging from the trained network, we find that the quantum state driven by driving pulse has bond-spin-density wave (BSDW) order for U\lesssim2VU\lesssim2V, where UU (VV) is the on-site (nearest-neighbor) Coulomb interaction. We separately calculate the time evolution of order parameters and find that the order parameters of BSDW are actually enhanced by photoexcitation as predicted by ML. Interestingly, the enhancement of the BSDW order in the photoexcited states for the 1DEHM has never been reported previously, despite the extensive studies so far, thus demonstrating the advantage of ML to assist characterizing photoexcited quantum states.
        ? Less",https://arxiv.org/pdf/1901.07900
6,Predicting the Results of LTL Model Checking using Multiple Machine Learning Algorithms,"['Weijun Zhu', 'Mingliang Xu']","In this paper, we study how to predict the results of LTL model checking using some machine learning algorithms. Some Kripke structures and LTL formulas and their model checking results are made up data set. The approaches based on the Random Forest (RF), K-Nearest Neighbors (KNN), Decision tree (DT), and Logistic Regression (LR) are used to training and prediction. The experiment results show that the average computation efficiencies of the RF, LR, DT, and KNN-based approaches are 2066181, 2525333, 1894000 and 294 times than that of the existing approach, respectively.
        ? Less",https://arxiv.org/pdf/1901.07891
7,Micro-UAV Detection and Classification from RF Fingerprints Using Machine Learning Techniques,"['Martins Ezuma', 'Fatih Erden', 'Chethan Kumar Anjinappa', 'Ozgur Ozdemir', 'Ismail Guvenc']","This paper focuses on the detection and classification of micro-unmanned aerial vehicles (UAVs) using radio frequency (RF) fingerprints of the signals transmitted from the controller to the micro-UAV. In the detection phase, raw signals are split into frames and transformed into the wavelet domain. A Markov models-based naive Bayes approach is used to check for the presence of a UAV in each frame. In the classification phase, unlike the traditional approaches that rely solely on time-domain signals and corresponding features, the proposed technique uses the energy transient signal. This approach is more robust to noise and can cope with different modulation techniques. First, the normalized energy trajectory is generated from the energy-time-frequency distribution of the raw control signal. Next, the start and end points of the energy transient are detected by searching for the most abrupt changes in the mean of the energy trajectory. Then, a set of statistical features is extracted from the energy transient. Significant features are selected by performing neighborhood component analysis (NCA) to keep the computational cost of the algorithm low. Finally, selected features are fed to several machine learning algorithms for classification. The algorithms are evaluated experimentally using a database containing 100 RF signals from each of 14 different UAV controllers. The signals are recorded wirelessly using a high-frequency oscilloscope. The data set is randomly partitioned into training and test sets for validation with the ratio 4:1. Ten Monte Carlo simulations are run and results are averaged to assess the performance of the methods. All the micro-UAVs are detected correctly and an average accuracy of 96.3% is achieved using the k-nearest neighbor (kNN) classification. Proposed methods are also tested for different signal-to-noise ratio (SNR) levels and results are reported.
        ? Less",https://arxiv.org/pdf/1901.07703
8,What Can Machine Learning Teach Us about Communications?,"['Mengke Lian', 'Christian Häger', 'Henry D. Pfister']","Rapid improvements in machine learning over the past decade are beginning to have far-reaching effects. For communications, engineers with limited domain expertise can now use off-the-shelf learning packages to design high-performance systems based on simulations. Prior to the current revolution in machine learning, the majority of communication engineers were quite aware that system parameters (such as filter coefficients) could be learned using stochastic gradient descent. It was not at all clear, however, that more complicated parts of the system architecture could be learned as well. In this paper, we discuss the application of machine-learning techniques to two communications problems and focus on what can be learned from the resulting systems. We were pleasantly surprised that the observed gains in one example have a simple explanation that only became clear in hindsight. In essence, deep learning discovered a simple and effective strategy that had not been considered earlier.
        ? Less",https://arxiv.org/pdf/1901.07592
9,A GFML-based Robot Agent for Human and Machine Cooperative Learning on Game of Go,"['Chang-Shing Lee', 'Mei-Hui Wang', 'Li-Chuang Chen', 'Yusuke Nojima', 'Tzong-Xiang Huang', 'Jinseok Woo', 'Naoyuki Kubota', 'Eri Sato-Shimokawara', 'Toru Yamaguchi']","This paper applies a genetic algorithm and fuzzy markup language to construct a human and smart machine cooperative learning system on game of Go. The genetic fuzzy markup language (GFML)-based Robot Agent can work on various kinds of robots, including Palro, Pepper, and TMUs robots. We use the parameters of FAIR open source Darkforest and OpenGo AI bots to construct the knowledge base of Open Go Darkforest (OGD) cloud platform for student learning on the Internet. In addition, we adopt the data from AlphaGo Master sixty online games as the training data to construct the knowledge base and rule base of the co-learning system. First, the Darkforest predicts the win rate based on various simulation numbers and matching rates for each game on OGD platform, then the win rate of OpenGo is as the final desired output. The experimental results show that the proposed approach can improve knowledge base and rule base of the prediction ability based on Darkforest and OpenGo AI bot with various simulation numbers.
        ? Less",https://arxiv.org/pdf/1901.07191
10,Machine and Deep Learning Applied to Galaxy Morphology - A Complete Classification Catalog,"['P. H. Barchi', 'R. R. de Carvalho', 'R. R. Rosa', 'R. Sautter', 'M. Soares-Santos', 'B. A. D. Marques', 'E. Clua']","Morphological classification is a key piece of information to define samples of galaxies aiming to study the large-scale structure of the universe. In essence, the challenge is to build up a robust methodology to perform a reliable morphological estimate from galaxy images. Here, we investigate how to substantially improve the galaxy classification within large datasets by mimicking human classification. We combine accurate visual classifications from the Galaxy Zoo project with machine and deep learning methodologies. We propose two distinct approaches for galaxy morphology: one based on non-parametric morphology and traditional machine learning algorithms; and another based on Deep Learning. To measure the input features for the traditional machine learning methodology, we have developed a system called CyMorph, with a novel non-parametric approach to study galaxy morphology. The main dataset employed comes from the Sloan Digital Sky Survey Data Release 7 (SDSS-DR7). We also discuss the class imbalance problem considering three classes. Performance of each model is mainly measured by Overall Accuracy (OA). A spectroscopic validation with astrophysical parameters is also provided for Decision Tree models to assess the quality of our morphological classification. In all of our samples, both Deep and Traditional Machine Learning approaches have over 94.5% OA to classify galaxies among 2 classes (elliptical and spiral galaxies). We provide a catalog with ~670,000 galaxies containing our best results, including morphological metrics and classification (supplementary data link). We compare our classification with state-of-art morphological classification from literature.
        ? Less",https://arxiv.org/pdf/1901.07047
11,Predicting wind pressures around circular cylinders using machine learning techniques,"['Gang Hu', 'K. C. S. Kwok']","Numerous studies have been carried out to measure wind pressures around circular cylinders since the early 20th century due to its engineering significance. Consequently, a large amount of wind pressure data sets have accumulated, which presents an excellent opportunity for using machine learning (ML) techniques to train models to predict wind pressures around circular cylinders. Wind pressures around smooth circular cylinders are a function of mainly the Reynolds number (Re), turbulence intensity (Ti) of the incident wind, and circumferential angle of the cylinder. Considering these three parameters as the inputs, this study trained two ML models to predict mean and fluctuating pressures respectively. Three machine learning algorithms including decision tree regressor, random forest, and gradient boosting regression trees (GBRT) were tested. The GBRT models exhibited the best performance for predicting both mean and fluctuating pressures, and they are capable of making accurate predictions for Re ranging from 10^4 to 10^6 and Ti ranging from 0% to 15%. It is believed that the GBRT models provide very efficient and economical alternative to traditional wind tunnel tests and computational fluid dynamic simulations for determining wind pressures around smooth circular cylinders within the studied Re and Ti range.
        ? Less",https://arxiv.org/pdf/1901.06752
12,Mixed Formal Learning: A Path to Transparent Machine Learning,['Sandra Carrico'],"This paper presents Mixed Formal Learning, a new architecture that learns models based on formal mathematical representations of the domain of interest and exposes latent variables. The second element in the architecture learns a particular skill, typically by using traditional prediction or classification mechanisms. Our key findings include that this architecture: (1) Facilitates transparency by exposing key latent variables based on a learned mathematical model; (2) Enables Low Shot and Zero Shot training of machine learning without sacrificing accuracy or recall.
        ? Less",https://arxiv.org/pdf/1901.06622
13,Molecular Force Fields with Gradient-Domain Machine Learning: Construction and Application to Dynamics of Small Molecules with Coupled Cluster Forces,"['Huziel E. Sauceda', 'Stefan Chmiela', 'Igor Poltavsky', 'Klaus-Robert Müller', 'Alexandre Tkatchenko']","We present the construction of molecular force fields for small molecules (less than 25 atoms) using the recently developed symmetrized gradient-domain machine learning (sGDML) approach [Chmiela et al., Nat. Commun. 9, 3887 (2018); Sci. Adv. 3, e1603015 (2017)]. This approach is able to accurately reconstruct complex high-dimensional potential-energy surfaces from just a few 100s of molecular conformations extracted from ab initio molecular dynamics trajectories. The data efficiency of the sGDML approach implies that atomic forces for these conformations can be computed with high-level wavefunction-based approaches, such as the ""gold standard"" CCSD(T) method. We demonstrate that the flexible nature of the sGDML model recovers local and non-local electronic interactions (e.g. H-bonding, proton transfer, lone pairs, changes in hybridization states, steric repulsion and n\to?^*n\to?^* interactions) without imposing any restriction on the nature of interatomic potentials. The analysis of sGDML molecular dynamics trajectories yields new qualitative insights into dynamics and spectroscopy of small molecules close to spectroscopic accuracy.
        ? Less",https://arxiv.org/pdf/1901.06594
14,Machine Learning with Clos Networks,"['Timothy Whithing', 'Thiam Khean Hah']","We present a new methodology for improving the accuracy of small neural networks by applying the concept of a clos network to achieve maximum expression in a smaller network. We explore the design space to show that more layers is beneficial, given the same number of parameters. We also present findings on how the relu nonlinearity ffects accuracy in separable networks. We present results on early work with Cifar-10 dataset.
        ? Less",https://arxiv.org/pdf/1901.06433
15,Protein Classification using Machine Learning and Statistical Techniques: A Comparative Analysis,"['Chhote Lal Prasad Gupta', 'Anand Bihari', 'Sudhakar Tripathi']","In recent era prediction of enzyme class from an unknown protein is one of the challenging tasks in bioinformatics. Day to day the number of proteins is increases as result the prediction of enzyme class gives a new opportunity to bioinformatics scholars. The prime objective of this article is to implement the machine learning classification technique for feature selection and predictions also find out an appropriate classification technique for function prediction. In this article the seven different classification technique like CRT, QUEST, CHAID, C5.0, ANN (Artificial Neural Network), SVM and Bayesian has been implemented on 4368 protein data that has been extracted from UniprotKB databank and categories into six different class. The proteins data is high dimensional sequence data and contain a maximum of 48 features.To manipulate the high dimensional sequential protein data with different classification technique, the SPSS has been used as an experimental tool. Different classification techniques give different results for every model and shows that the data are imbalanced for class C4, C5 and C6. The imbalanced data affect the performance of model. In these three classes the precision and recall value is very less or negligible. The experimental results highlight that the C5.0 classification technique accuracy is more suited for protein feature classification and predictions. The C5.0 classification technique gives 95.56% accuracy and also gives high precision and recall value. Finally, we conclude that the features that is selected can be used for function prediction.
        ? Less",https://arxiv.org/pdf/1901.06152
16,Optimization Models for Machine Learning: A Survey,"['Claudio Gambella', 'Bissan Ghaddar', 'Joe Naoum-Sawaya']","This paper surveys the machine learning literature and presents machine learning as optimization models. Such models can benefit from the advancement of numerical optimization techniques which have already played a distinctive role in several machine learning settings. Particularly, mathematical optimization models are presented for commonly used machine learning approaches for regression, classification, clustering, and deep neural networks as well new emerging applications in machine teaching and empirical model learning. The strengths and the shortcomings of these models are discussed and potential research directions are highlighted.
        ? Less",https://arxiv.org/pdf/1901.05331
17,Context Aware Machine Learning,['Yun Zeng'],"We propose a principle for exploring context in machine learning models. Starting with a simple assumption that each observation may or may not depend on its context, a conditional probability distribution is decomposed into two parts: context-free and context-sensitive. Then by employing the log-linear word production model for relating random variables to their embedding space representation and making use of the convexity of natural exponential function, we show that the embedding of an observation can also be decomposed into a weighted sum of two vectors, representing its context-free and context-sensitive parts, respectively. This simple treatment of context provides a unified view of many existing deep learning models, leading to revisions of these models able to achieve significant performance boost. Specifically, our upgraded version of a recent sentence embedding model not only outperforms the original one by a large margin, but also leads to a new, principled approach for compositing the embeddings of bag-of-words features, as well as a new architecture for modeling attention in deep neural networks. More surprisingly, our new principle provides a novel understanding of the gates and equations defined by the long short term memory model, which also leads to a new model that is able to converge significantly faster and achieve much lower prediction errors. Furthermore, our principle also inspires a new type of generic neural network layer that better resembles real biological neurons than the traditional linear mapping plus nonlinear activation based architecture. Its multi-layer extension provides a new principle for deep neural networks which subsumes residual network (ResNet) as its special case, and its extension to convolutional neutral network model accounts for irrelevant input (e.g., background in an image) in addition to filtering.
        ? Less",https://arxiv.org/pdf/1901.03415
18,Machine learning assisted measurement of local topological invariants,"['Marcello D. Caio', 'Marco Caccin', 'Paul Baireuther', 'Timo Hyart', 'Michel Fruchart']","The continuous effort towards topological quantum devices calls for an efficient and non-invasive method to assess the conformity of components in different topological phases. Here, we show that machine learning paves the way towards non-invasive topological quality control. To do so, we use a local topological marker, able to discriminate between topological phases of one-dimensional wires. The direct observation of this marker in solid state systems is challenging, but we show that an artificial neural network can learn to approximate it from the experimentally accessible local density of states. Our method distinguishes different non-trivial phases, even for systems where direct transport measurements are not available and for composite systems. This new approach could find significant use in experiments, ranging from the study of novel topological materials to high-throughput automated material design.
        ? Less",https://arxiv.org/pdf/1901.03346
19,A New Perspective on Machine Learning: How to do Perfect Supervised Learning,['Hui Jiang'],"In this work, we introduce the concept of bandlimiting into the theory of machine learning because all physical processes are bandlimited by nature, including real-world machine learning tasks. After the bandlimiting constraint is taken into account, our theoretical analysis has shown that all practical machine learning tasks are asymptotically solvable in a perfect sense. Furthermore, the key towards this solvability almost solely relies on two factors: i) a sufficiently large amount of training samples beyond a threshold determined by a difficulty measurement of the underlying task; ii) a sufficiently complex model that is properly bandlimited. Moreover, for some special cases, we have derived new error bounds for perfect learning, which can quantify the difficulty of learning. These case-specific bounds are much tighter than the uniform bounds in conventional learning theory. Our results have provided a new perspective to explain the recent successes of large-scale supervised learning using complex models like neural networks.
        ? Less",https://arxiv.org/pdf/1901.02046
20,Natively Interpretable Machine Learning and Artificial Intelligence: Preliminary Results and Future Directions,"['Christopher J. Hazard', 'Christopher Fusting', 'Michael Resnick', 'Michael Auerbach', 'Michael Meehan', 'Valeri Korobov']","Machine learning models have become more and more complex in order to better approximate complex functions. Although fruitful in many domains, the added complexity has come at the cost of model interpretability. The once popular k-nearest neighbors (kNN) approach, which finds and uses the most similar data for reasoning, has received much less attention in recent decades due to numerous problems when compared to other techniques. We show that many of these historical problems with kNN can be overcome, and our contribution has applications not only in machine learning but also in online learning, data synthesis, anomaly detection, model compression, and reinforcement learning, without sacrificing interpretability. We introduce a synthesis between kNN and information theory that we hope will provide a clear path towards models that are innately interpretable and auditable. Through this work we hope to gather interest in combining kNN with information theory as a promising path to fully auditable machine learning and artificial intelligence.
        ? Less",https://arxiv.org/pdf/1901.00246
21,Machine learning materials physics: Integrable deep neural networks enable scale bridging by learning free energy functions,"['G. H. Teichert', 'A. R. Natarajan', 'A. Van der Ven', 'K. Garikipati']","The free energy of a system is central to many material models. Although free energy data is not generally found directly, its derivatives can be observed or calculated. In this work, we present an Integrable Deep Neural Network (IDNN) that can be trained to derivative data, then analytically integrated to recover an accurate representation of the free energy. The IDNN is demonstrated by training to the chemical potential values of a binary alloy with B2 ordering. The resulting DNN representation of the free energy is used in a phase field simulation and found to predict the appropriate formation of antiphase boundaries in the material. In contrast, a B-spline representation of the same data failed to represent the physics of the system with sufficient fidelity to resolve the antiphase boundaries.
        ? Less",https://arxiv.org/pdf/1901.00081
22,Using Apple Machine Learning Algorithms to Detect and Subclassify Non-Small Cell Lung Cancer,"['Andrew A. Borkowski', 'Catherine P. Wilson', 'Steven A. Borkowski', 'Lauren A. Deland', 'Stephen M. Mastorides']","Lung cancer continues to be a major healthcare challenge with high morbidity and mortality rates among both men and women worldwide. The majority of lung cancer cases are of non-small cell lung cancer type. With the advent of targeted cancer therapy, it is imperative not only to properly diagnose but also sub-classify non-small cell lung cancer. In our study, we evaluated the utility of using Apple Create ML module to detect and sub-classify non-small cell carcinomas based on histopathological images. After module optimization, the program detected 100% of non-small cell lung cancer images and successfully subclassified the majority of the images. Trained modules, such as ours, can be utilized in diagnostic smartphone-based applications, augmenting diagnostic services in understaffed areas of the world.
        ? Less",https://arxiv.org/pdf/1808.08230
23,Machine Learning Estimators for Lattice QCD Observables,"['Boram Yoon', 'Tanmoy Bhattacharya', 'Rajan Gupta']","A novel technique using machine learning (ML) to reduce the computational cost of evaluating lattice quantum chromodynamics (QCD) observables is presented. The ML is trained on a subset of background gauge field configurations, called the labeled set, to predict an observable OO from the values of correlated, but less compute-intensive, observables \mathbf{X}\mathbf{X} calculated on the full sample. By using a second subset, also part of the labeled set, we estimate the bias in the result predicted by the trained ML algorithm. The bias-corrected final estimate of the expectation value of OO, obtained by running the ML algorithm on the remaining unlabeled set, is improved by combining with the labeled data. A reduction in the computational cost by about 35% is demonstrated for two different lattice QCD calculations using the Boosted decision tree (BDT) regression ML algorithm: (1) prediction of the nucleon three-point correlation functions that yield isovector charges from the two-point correlation functions, and (2) prediction of the phase acquired by the neutron mass when a small Charge-Parity (CP) violating interaction, the quark chromo-electric dipole moment interaction, is added to QCD, again from the two-point correlation functions calculated without CP violation.
        ? Less",https://arxiv.org/pdf/1807.05971
24,Stellar formation rates in galaxies using Machine Learning models,"['Michele Delli Veneri', 'Stefano Cavuoti', 'Massimo Brescia', 'Giuseppe Riccio', 'Giuseppe Longo']","Global Stellar Formation Rates or SFRs are crucial to constrain theories of galaxy formation and evolution. SFR's are usually estimated via spectroscopic observations which require too much previous telescope time and therefore cannot match the needs of modern precision cosmology. We therefore propose a novel method to estimate SFRs for large samples of galaxies using a variety of supervised ML models.
        ? Less",https://arxiv.org/pdf/1805.06338
25,Machine Learning Modeling of Wigner Intracule Functionals for Two Electrons in One Dimension,"['Rutvij Vihang Bhavsar', 'Raghunathan Ramakrishnan']","In principle, many-electron correlation energy can be precisely computed from a reduced Wigner distribution function (\mathcal{W}\mathcal{W}) thanks to a universal functional transformation (\mathcal{F}\mathcal{F}), whose formal existence is akin to that of the exchange-correlation functional in density functional theory. While the exact dependence of \mathcal{F}\mathcal{F} on \mathcal{W}\mathcal{W} is unknown, a few approximate parametric models have been proposed in the past. Here, for a dataset of 923 one-dimensional external potentials with two interacting electrons, we apply machine learning to model \mathcal{F}\mathcal{F} within the kernel Ansatz. We deal with over-fitting of the kernel to a specific region of phase-space by a one-step regularization not depending on any hyperparameters. Reference correlation energies have been computed by performing exact and Hartree--Fock calculations using discrete variable representation. The resulting models require \mathcal{W}\mathcal{W} calculated at the Hartree--Fock level as input while yielding monotonous decay in the predicted correlation energies of new molecules reaching sub-chemical accuracy with training.
        ? Less",https://arxiv.org/pdf/1802.00873
